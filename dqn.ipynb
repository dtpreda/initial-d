{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import CarRacingAgent\n",
    "import gym\n",
    "from collections import deque\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "START_EPISODE = 0\n",
    "END_EPISODE = 4000\n",
    "SKIP_FRAMES = 2\n",
    "BATCH_SIZE = 32\n",
    "SAVE_MODEL_EVERY = 25\n",
    "UPDATE_TARGET_MODEL_EVERY = 5\n",
    "NEGATIVE_REWARD_TOLERANCE_WINDOW = 50\n",
    "NEGATIVE_REWARD_TOLERANCE_LIMIT = 25\n",
    "\n",
    "CONTINUE_TRAINING = False\n",
    "\n",
    "folder_name = './overnightRun/'\n",
    "SAVE_MODEL_PATH = './overnightRun/episode_{}.h5'\n",
    "\n",
    "# create directory to save models\n",
    "import os\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('CarRacing-v2', render_mode='human')\n",
    "RENDER = False\n",
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards_over_episodes = []\n",
    "average_rewards = []\n",
    "epsilon_values = []\n",
    "if CONTINUE_TRAINING:\n",
    "    agent = CarRacingAgent()\n",
    "    agent.model.load_weights(folder_name + 'episode_{}.h5'.format(START_EPISODE))\n",
    "    with open(folder_name + f'episode_{START_EPISODE}.txt', 'r') as f:\n",
    "        data = eval(f.read())\n",
    "        rewards_over_episodes = data['rewards_over_episodes']\n",
    "        epsilon_values = data['epsilon_values']\n",
    "else:\n",
    "    agent = CarRacingAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(state):\n",
    "    state = cv.cvtColor(state, cv.COLOR_RGB2GRAY)\n",
    "    state = state.astype(np.float32)\n",
    "    state /= 255.0\n",
    "    return state\n",
    "\n",
    "def save(episode, rewards_over_episodes):\n",
    "    data = {\n",
    "        'rewards_over_episodes': rewards_over_episodes,\n",
    "        **agent.get_params()\n",
    "    }\n",
    "    h5file = folder_name + f'{episode}.h5'\n",
    "    data_file_path = folder_name + f'episode_{episode}.txt'\n",
    "    data_file = open(data_file_path, 'w')\n",
    "    data_file.write(str(data))\n",
    "    agent.save(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deque_to_stack(deque):\n",
    "    frame_stack = np.array(deque)\n",
    "    # to channels last\n",
    "    return np.transpose(frame_stack, (1, 2, 0))\n",
    "\n",
    "def get_average(episodes):\n",
    "    if len(episodes) == 0:\n",
    "        return 0\n",
    "    return sum(episodes) / len(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(START_EPISODE, END_EPISODE + 1):\n",
    "    print(f\"Starting Episode: {episode}/{END_EPISODE}\")\n",
    "    initial_state, obs_info = env.reset()\n",
    "    initial_state = process_state(initial_state)\n",
    "    \n",
    "    total_reward = 0\n",
    "    negative_reward_count = 0\n",
    "    state_stack = deque([initial_state]*agent.frame_stack_num,maxlen=agent.frame_stack_num)\n",
    "    time_frame_counter = 1\n",
    "    done = False\n",
    "    agent.memory = agent.build_memory(BATCH_SIZE)\n",
    "    while True:\n",
    "        #if RENDER:\n",
    "        #    env.render()\n",
    "\n",
    "        current_state_stack = deque_to_stack(state_stack)\n",
    "        action = agent.act(np.expand_dims(current_state_stack, 0))\n",
    "\n",
    "        reward = 0\n",
    "        for _ in range(SKIP_FRAMES + 1):\n",
    "            next_state, step_reward, done, _, _ = env.step(action)\n",
    "            reward += step_reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "    \n",
    "        negative_reward_count += 1 if time_frame_counter > NEGATIVE_REWARD_TOLERANCE_WINDOW and reward < 0 else 0\n",
    "\n",
    "        # we can increase rewards here for specific actions to encourage the agent to learn them\n",
    "\n",
    "        if action[0] > 0 and action[0] == 0:\n",
    "            reward *= (1.0 + action[0])\n",
    "\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = process_state(next_state)\n",
    "        state_stack.append(next_state)\n",
    "        next_state_stack = deque_to_stack(state_stack)\n",
    "\n",
    "        \n",
    "\n",
    "        agent.remember(current_state_stack, action, reward, next_state_stack, done)\n",
    "\n",
    "        if done or negative_reward_count > NEGATIVE_REWARD_TOLERANCE_LIMIT or total_reward < 0:\n",
    "            print(f\"Episode: {episode}/{END_EPISODE}, Total Reward: {total_reward:.2}, Epsilon: {agent.epsilon:.2}\")\n",
    "            rewards_over_episodes.append(total_reward)\n",
    "            average_rewards.append(np.mean(total_reward/time_frame_counter))\n",
    "            epsilon_values.append(agent.epsilon)\n",
    "            break\n",
    "        \n",
    "\n",
    "        if len(agent.memory) == BATCH_SIZE:\n",
    "            agent.replay(BATCH_SIZE)\n",
    "            \n",
    "        time_frame_counter += 1\n",
    "\n",
    "        if time_frame_counter % 50 == 0:\n",
    "            print(f\"Episode: {episode}/{END_EPISODE}, Iteration:{time_frame_counter}, Total Reward: {total_reward:.2}, Epsilon: {agent.epsilon:.2}\")\n",
    "    \n",
    "    \n",
    "    if episode % UPDATE_TARGET_MODEL_EVERY == 0:\n",
    "        agent.update_target_model()\n",
    "    \n",
    "    if episode % SAVE_MODEL_EVERY == 0:\n",
    "        save(episode, rewards_over_episodes)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot rewards over episodes\n",
    "import matplotlib.pyplot as plt\n",
    "print(len(rewards_over_episodes))\n",
    "plt.plot(list(range(len(rewards_over_episodes))), rewards_over_episodes, color='blue')\n",
    "\n",
    "#plot moving average\n",
    "moving_average = []\n",
    "for i in range(len(rewards_over_episodes)):\n",
    "    if i < 50:\n",
    "        moving_average.append(np.mean(rewards_over_episodes[:i+1]))\n",
    "    else:\n",
    "        moving_average.append(np.mean(rewards_over_episodes[i-50:i+1]))\n",
    "    \n",
    "plt.plot(list(range(len(rewards_over_episodes))), moving_average, color='red')\n",
    "plt.show()\n",
    "plt.plot(list(range(len(epsilon_values))), epsilon_values, color='blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', render_mode='human')\n",
    "episode = 900\n",
    "ai_agent = CarRacingAgent()\n",
    "ai_agent.load('./save/episode_900.h5')\n",
    "\n",
    "for e in range(episode):\n",
    "    init_state, obs_info = env.reset()\n",
    "    init_state = process_state(init_state)\n",
    "\n",
    "    total_reward = 0\n",
    "    punishment_counter = 0\n",
    "    state_frame_stack_queue = deque([init_state]*ai_agent.frame_stack_num, maxlen=ai_agent.frame_stack_num)\n",
    "    time_frame_counter = 1\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        current_state_frame_stack = deque_to_stack(state_frame_stack_queue)\n",
    "        act_values = ai_agent.model.predict(current_state_frame_stack, verbose=0)\n",
    "        action = ai_agent.action_space[np.argmax(act_values[0])]\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = process_state(next_state)\n",
    "        state_frame_stack_queue.append(next_state)\n",
    "\n",
    "        if done:\n",
    "            print('Episode: {}/{}, Scores(Time Frames): {}, Total Rewards: {:.2}'.format(e+1, 400, time_frame_counter, float(total_reward)))\n",
    "            break\n",
    "        time_frame_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
