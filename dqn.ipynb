{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import CarRacingAgent\n",
    "import gym\n",
    "from collections import deque\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = False\n",
    "START_EPISODE = 1\n",
    "END_EPISODE = 150\n",
    "SKIP_FRAMES = 2\n",
    "BATCH_SIZE = 64\n",
    "SAVE_MODEL_EVERY = 25\n",
    "UPDATE_TARGET_MODEL_EVERY = 5\n",
    "NEGATIVE_REWARD_TOLERANCE_WINDOW = 100\n",
    "NEGATIVE_REWARD_TOLERANCE_LIMIT = 25\n",
    "SAVE_MODEL_PATH = './save/episode_{}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('CarRacing-v2', render_mode='human')\n",
    "env = gym.make('CarRacing-v2')\n",
    "agent = CarRacingAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(state):\n",
    "    state = cv.cvtColor(state, cv.COLOR_RGB2GRAY)\n",
    "    state = state.astype(np.float32)\n",
    "    state /= 255.0\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deque_to_stack(deque):\n",
    "    frame_stack = np.array(deque)\n",
    "    # to channels last\n",
    "    return np.transpose(frame_stack, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_over_episodes = []\n",
    "for episode in range(START_EPISODE, END_EPISODE + 1):\n",
    "    print(f\"Starting Episode: {episode}/{END_EPISODE}\")\n",
    "    initial_state, obs_info = env.reset()\n",
    "    initial_state = process_state(initial_state)\n",
    "    \n",
    "    total_reward = 0\n",
    "    negative_reward_count = 0\n",
    "    state_stack = deque([initial_state]*agent.frame_stack_num,maxlen=agent.frame_stack_num)\n",
    "    time_frame_counter = 1\n",
    "    done = False\n",
    "    agent.memory = agent.build_memory()\n",
    "    while True:\n",
    "        #if RENDER:\n",
    "        #    env.render()\n",
    "        \n",
    "        current_state_stack = deque_to_stack(state_stack)\n",
    "        action = agent.act(np.expand_dims(current_state_stack, 0))\n",
    "\n",
    "        reward = 0\n",
    "        for _ in range(SKIP_FRAMES + 1):\n",
    "            next_state, step_reward, done, _, _ = env.step(action)\n",
    "            reward += step_reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        negative_reward_count += 1 if time_frame_counter > NEGATIVE_REWARD_TOLERANCE_WINDOW and reward < 0 else 0\n",
    "\n",
    "        # we can increase rewards here for specific actions to encourage the agent to learn them\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = process_state(next_state)\n",
    "        state_stack.append(next_state)\n",
    "        next_state_stack = deque_to_stack(state_stack)\n",
    "\n",
    "        agent.remember(current_state_stack, action, reward, next_state_stack, done)\n",
    "\n",
    "        if done or negative_reward_count > NEGATIVE_REWARD_TOLERANCE_LIMIT or total_reward < 0:\n",
    "            print(f\"Episode: {episode}/{END_EPISODE}, Total Reward: {total_reward:.2}, Epsilon: {agent.epsilon:.2}\")\n",
    "            rewards_over_episodes.append(total_reward)\n",
    "            break\n",
    "\n",
    "        if len(agent.memory) == BATCH_SIZE:\n",
    "            print(\"Starting Training\")\n",
    "        \n",
    "        if len(agent.memory) > BATCH_SIZE:\n",
    "            agent.replay(BATCH_SIZE)\n",
    "        time_frame_counter += 1\n",
    "\n",
    "        if time_frame_counter % 50 == 0:\n",
    "            print(f\"Episode: {episode}/{END_EPISODE}, Iteration:{time_frame_counter}, Total Reward: {total_reward:.2}, Epsilon: {agent.epsilon:.2}\")\n",
    "    \n",
    "    if episode % UPDATE_TARGET_MODEL_EVERY == 0:\n",
    "        agent.update_target_model()\n",
    "    \n",
    "    if episode % SAVE_MODEL_EVERY == 0:\n",
    "        agent.save(SAVE_MODEL_PATH.format(episode))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot rewards over episodes\n",
    "import matplotlib.pyplot as plt\n",
    "print(rewards_over_episodes)\n",
    "plt.plot(list(range(len(rewards_over_episodes))), rewards_over_episodes)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
