{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import CarRacingAgent\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "START_EPISODE = 0\n",
    "END_EPISODE = 4000\n",
    "SKIP_FRAMES = 2\n",
    "BATCH_SIZE = 32\n",
    "SAVE_MODEL_EVERY = 25\n",
    "UPDATE_TARGET_MODEL_EVERY = 5\n",
    "NEGATIVE_REWARD_TOLERANCE_WINDOW = 50\n",
    "NEGATIVE_REWARD_TOLERANCE_LIMIT = 25\n",
    "\n",
    "CONTINUE_TRAINING = False\n",
    "\n",
    "folder_name = './overnightRun/'\n",
    "SAVE_MODEL_PATH = './overnightRun/episode_{}.h5'\n",
    "\n",
    "# create directory to save models\n",
    "import os\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('CarRacing-v2', render_mode='human')\n",
    "RENDER = False\n",
    "env = gym.make('CarRacing-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards_over_episodes = []\n",
    "average_rewards = []\n",
    "epsilon_values = []\n",
    "if CONTINUE_TRAINING:\n",
    "    agent = CarRacingAgent()\n",
    "    agent.model.load_weights(folder_name + 'episode_{}.h5'.format(START_EPISODE))\n",
    "    with open(folder_name + f'episode_{START_EPISODE}.txt', 'r') as f:\n",
    "        data = eval(f.read())\n",
    "        rewards_over_episodes = data['rewards_over_episodes']\n",
    "        epsilon_values = data['epsilon_values']\n",
    "else:\n",
    "    agent = CarRacingAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(state):\n",
    "    state = cv.cvtColor(state, cv.COLOR_RGB2GRAY)\n",
    "    state = state.astype(np.float32)\n",
    "    state /= 255.0\n",
    "    return state\n",
    "\n",
    "def save(episode, rewards_over_episodes):\n",
    "    data = {\n",
    "        'rewards_over_episodes': rewards_over_episodes,\n",
    "        **agent.get_params()\n",
    "    }\n",
    "    h5file = folder_name + f'{episode}.h5'\n",
    "    data_file_path = folder_name + f'episode_{episode}.txt'\n",
    "    data_file = open(data_file_path, 'w')\n",
    "    data_file.write(str(data))\n",
    "    agent.save(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deque_to_stack(deque):\n",
    "    frame_stack = np.array(deque)\n",
    "    # to channels last\n",
    "    return np.transpose(frame_stack, (1, 2, 0))\n",
    "\n",
    "def get_average(episodes):\n",
    "    if len(episodes) == 0:\n",
    "        return 0\n",
    "    return sum(episodes) / len(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(START_EPISODE, END_EPISODE + 1):\n",
    "    print(f\"Starting Episode: {episode}/{END_EPISODE}\")\n",
    "    initial_state, obs_info = env.reset()\n",
    "    initial_state = process_state(initial_state)\n",
    "    \n",
    "    total_reward = 0\n",
    "    negative_reward_count = 0\n",
    "    state_stack = deque([initial_state]*agent.frame_stack_num,maxlen=agent.frame_stack_num)\n",
    "    time_frame_counter = 1\n",
    "    done = False\n",
    "    agent.memory = agent.build_memory(BATCH_SIZE)\n",
    "    while True:\n",
    "        #if RENDER:\n",
    "        #    env.render()\n",
    "\n",
    "        current_state_stack = deque_to_stack(state_stack)\n",
    "        action = agent.act(np.expand_dims(current_state_stack, 0))\n",
    "\n",
    "        reward = 0\n",
    "        for _ in range(SKIP_FRAMES + 1):\n",
    "            next_state, step_reward, done, _, _ = env.step(action)\n",
    "            reward += step_reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "    \n",
    "        negative_reward_count += 1 if time_frame_counter > NEGATIVE_REWARD_TOLERANCE_WINDOW and reward < 0 else 0\n",
    "\n",
    "        # we can increase rewards here for specific actions to encourage the agent to learn them\n",
    "\n",
    "        if action[0] > 0 and action[0] == 0:\n",
    "            reward *= (1.0 + action[0])\n",
    "\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = process_state(next_state)\n",
    "        state_stack.append(next_state)\n",
    "        next_state_stack = deque_to_stack(state_stack)\n",
    "\n",
    "        \n",
    "\n",
    "        agent.remember(current_state_stack, action, reward, next_state_stack, done)\n",
    "\n",
    "        if done or negative_reward_count > NEGATIVE_REWARD_TOLERANCE_LIMIT or total_reward < 0:\n",
    "            print(f\"Episode: {episode}/{END_EPISODE}, Total Reward: {total_reward:.2}, Epsilon: {agent.epsilon:.2}\")\n",
    "            rewards_over_episodes.append(total_reward)\n",
    "            average_rewards.append(np.mean(total_reward/time_frame_counter))\n",
    "            epsilon_values.append(agent.epsilon)\n",
    "            break\n",
    "        \n",
    "\n",
    "        if len(agent.memory) == BATCH_SIZE:\n",
    "            agent.replay(BATCH_SIZE)\n",
    "            \n",
    "        time_frame_counter += 1\n",
    "\n",
    "        if time_frame_counter % 50 == 0:\n",
    "            print(f\"Episode: {episode}/{END_EPISODE}, Iteration:{time_frame_counter}, Total Reward: {total_reward:.2}, Epsilon: {agent.epsilon:.2}\")\n",
    "    \n",
    "    \n",
    "    if episode % UPDATE_TARGET_MODEL_EVERY == 0:\n",
    "        agent.update_target_model()\n",
    "    \n",
    "    if episode % SAVE_MODEL_EVERY == 0:\n",
    "        save(episode, rewards_over_episodes)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot rewards over episodes\n",
    "import matplotlib.pyplot as plt\n",
    "print(len(rewards_over_episodes))\n",
    "plt.plot(list(range(len(rewards_over_episodes))), rewards_over_episodes, color='blue')\n",
    "\n",
    "#plot moving average\n",
    "moving_average = []\n",
    "for i in range(len(rewards_over_episodes)):\n",
    "    if i < 50:\n",
    "        moving_average.append(np.mean(rewards_over_episodes[:i+1]))\n",
    "    else:\n",
    "        moving_average.append(np.mean(rewards_over_episodes[i-50:i+1]))\n",
    "    \n",
    "plt.plot(list(range(len(rewards_over_episodes))), moving_average, color='red')\n",
    "plt.show()\n",
    "plt.plot(list(range(len(epsilon_values))), epsilon_values, color='blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m current_state_frame_stack \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(deque_to_stack(state_frame_stack_queue), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m action \u001b[39m=\u001b[39m ai_agent\u001b[39m.\u001b[39mact(current_state_frame_stack, explore\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m next_state, reward, done, info, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     27\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m     29\u001b[0m next_state \u001b[39m=\u001b[39m process_state(next_state)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/gymnasium/envs/box2d/car_racing.py:553\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld\u001b[39m.\u001b[39mStep(\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m FPS, \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m)\n\u001b[1;32m    551\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m FPS\n\u001b[0;32m--> 553\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render(\u001b[39m\"\u001b[39;49m\u001b[39mstate_pixels\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    555\u001b[0m step_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    556\u001b[0m terminated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/gymnasium/envs/box2d/car_racing.py:617\u001b[0m, in \u001b[0;36mCarRacing._render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    614\u001b[0m trans \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mVector2((scroll_x, scroll_y))\u001b[39m.\u001b[39mrotate_rad(angle)\n\u001b[1;32m    615\u001b[0m trans \u001b[39m=\u001b[39m (WINDOW_W \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m trans[\u001b[39m0\u001b[39m], WINDOW_H \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m+\u001b[39m trans[\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 617\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_road(zoom, trans, angle)\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcar\u001b[39m.\u001b[39mdraw(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf,\n\u001b[1;32m    620\u001b[0m     zoom,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m     mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mstate_pixels_list\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstate_pixels\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    624\u001b[0m )\n\u001b[1;32m    626\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mflip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, \u001b[39mFalse\u001b[39;00m, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/gymnasium/envs/box2d/car_racing.py:669\u001b[0m, in \u001b[0;36mCarRacing._render_road\u001b[0;34m(self, zoom, translation, angle)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[1;32m    668\u001b[0m     \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[0;32m--> 669\u001b[0m         grass\u001b[39m.\u001b[39;49mappend(\n\u001b[1;32m    670\u001b[0m             [\n\u001b[1;32m    671\u001b[0m                 (GRASS_DIM \u001b[39m*\u001b[39;49m x \u001b[39m+\u001b[39;49m GRASS_DIM, GRASS_DIM \u001b[39m*\u001b[39;49m y \u001b[39m+\u001b[39;49m \u001b[39m0\u001b[39;49m),\n\u001b[1;32m    672\u001b[0m                 (GRASS_DIM \u001b[39m*\u001b[39;49m x \u001b[39m+\u001b[39;49m \u001b[39m0\u001b[39;49m, GRASS_DIM \u001b[39m*\u001b[39;49m y \u001b[39m+\u001b[39;49m \u001b[39m0\u001b[39;49m),\n\u001b[1;32m    673\u001b[0m                 (GRASS_DIM \u001b[39m*\u001b[39;49m x \u001b[39m+\u001b[39;49m \u001b[39m0\u001b[39;49m, GRASS_DIM \u001b[39m*\u001b[39;49m y \u001b[39m+\u001b[39;49m GRASS_DIM),\n\u001b[1;32m    674\u001b[0m                 (GRASS_DIM \u001b[39m*\u001b[39;49m x \u001b[39m+\u001b[39;49m GRASS_DIM, GRASS_DIM \u001b[39m*\u001b[39;49m y \u001b[39m+\u001b[39;49m GRASS_DIM),\n\u001b[1;32m    675\u001b[0m             ]\n\u001b[1;32m    676\u001b[0m         )\n\u001b[1;32m    677\u001b[0m \u001b[39mfor\u001b[39;00m poly \u001b[39min\u001b[39;00m grass:\n\u001b[1;32m    678\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_draw_colored_polygon(\n\u001b[1;32m    679\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, poly, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrass_color, zoom, translation, angle\n\u001b[1;32m    680\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dqn import normal_space\n",
    "\n",
    "env = gym.make('CarRacing-v2', render_mode='human')\n",
    "episode = 900\n",
    "ai_agent = CarRacingAgent(action_space=normal_space)\n",
    "#print(ai_agent.model.summary())\n",
    "ai_agent.load('./save2/episode_1500.h5')\n",
    "\n",
    "#print(ai_agent.model.summary())\n",
    "\n",
    "for e in range(episode):\n",
    "    init_state, obs_info = env.reset()\n",
    "    init_state = process_state(init_state)\n",
    "\n",
    "    total_reward = 0\n",
    "    punishment_counter = 0\n",
    "    state_frame_stack_queue = deque([init_state]*ai_agent.frame_stack_num, maxlen=ai_agent.frame_stack_num)\n",
    "    time_frame_counter = 11\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        current_state_frame_stack = np.expand_dims(deque_to_stack(state_frame_stack_queue), axis=0)\n",
    "        action = ai_agent.act(current_state_frame_stack, explore=False)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = process_state(next_state)\n",
    "        state_frame_stack_queue.append(next_state)\n",
    "\n",
    "        if done:\n",
    "            print('Episode: {}/{}, Scores(Time Frames): {}, Total Rewards: {:.2}'.format(e+1, 400, time_frame_counter, float(total_reward)))\n",
    "            break\n",
    "        time_frame_counter += 1\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
